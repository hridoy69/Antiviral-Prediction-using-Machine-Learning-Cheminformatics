{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, mean_absolute_error, mean_squared_error, r2_score, roc_auc_score, cohen_kappa_score\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BCUTv-1l</th>\n",
       "      <th>BCUTi-1h</th>\n",
       "      <th>BCUTp-1l</th>\n",
       "      <th>BCUTZ-1l</th>\n",
       "      <th>BCUTd-1h</th>\n",
       "      <th>BCUTare-1l</th>\n",
       "      <th>BCUTs-1l</th>\n",
       "      <th>BCUTse-1l</th>\n",
       "      <th>BCUTpe-1l</th>\n",
       "      <th>PEOE_VSA9</th>\n",
       "      <th>...</th>\n",
       "      <th>piPC4</th>\n",
       "      <th>ETA_dEpsilon_D</th>\n",
       "      <th>GATS1se</th>\n",
       "      <th>SRW09</th>\n",
       "      <th>SMR_VSA6</th>\n",
       "      <th>AATS1i</th>\n",
       "      <th>MDEC-22</th>\n",
       "      <th>VR1_DzZ</th>\n",
       "      <th>VR3_Dzpe</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.701776</td>\n",
       "      <td>14.557741</td>\n",
       "      <td>0.744080</td>\n",
       "      <td>5.748825</td>\n",
       "      <td>3.218349</td>\n",
       "      <td>2.238180</td>\n",
       "      <td>1.240005</td>\n",
       "      <td>2.479451</td>\n",
       "      <td>2.284833</td>\n",
       "      <td>12.710848</td>\n",
       "      <td>...</td>\n",
       "      <td>4.754667</td>\n",
       "      <td>0.492929</td>\n",
       "      <td>0.839738</td>\n",
       "      <td>6.580639</td>\n",
       "      <td>6.606882</td>\n",
       "      <td>151.382734</td>\n",
       "      <td>2.249577</td>\n",
       "      <td>74.441211</td>\n",
       "      <td>4.780763</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14.700550</td>\n",
       "      <td>13.644287</td>\n",
       "      <td>0.741446</td>\n",
       "      <td>5.679853</td>\n",
       "      <td>3.258335</td>\n",
       "      <td>2.178923</td>\n",
       "      <td>1.143745</td>\n",
       "      <td>2.424758</td>\n",
       "      <td>2.228722</td>\n",
       "      <td>39.958147</td>\n",
       "      <td>...</td>\n",
       "      <td>5.547129</td>\n",
       "      <td>0.364441</td>\n",
       "      <td>0.878124</td>\n",
       "      <td>7.412764</td>\n",
       "      <td>34.729217</td>\n",
       "      <td>144.347348</td>\n",
       "      <td>3.452842</td>\n",
       "      <td>200.587788</td>\n",
       "      <td>6.399368</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.304117</td>\n",
       "      <td>17.424774</td>\n",
       "      <td>0.545839</td>\n",
       "      <td>5.666628</td>\n",
       "      <td>3.296716</td>\n",
       "      <td>2.162280</td>\n",
       "      <td>1.195097</td>\n",
       "      <td>2.406729</td>\n",
       "      <td>2.211106</td>\n",
       "      <td>30.297854</td>\n",
       "      <td>...</td>\n",
       "      <td>5.873877</td>\n",
       "      <td>0.391712</td>\n",
       "      <td>0.714820</td>\n",
       "      <td>6.555357</td>\n",
       "      <td>25.161346</td>\n",
       "      <td>145.867240</td>\n",
       "      <td>8.909374</td>\n",
       "      <td>264.575821</td>\n",
       "      <td>6.887221</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13.304118</td>\n",
       "      <td>17.424774</td>\n",
       "      <td>0.545842</td>\n",
       "      <td>5.666628</td>\n",
       "      <td>3.296579</td>\n",
       "      <td>2.162280</td>\n",
       "      <td>1.195097</td>\n",
       "      <td>2.406729</td>\n",
       "      <td>2.211106</td>\n",
       "      <td>30.297854</td>\n",
       "      <td>...</td>\n",
       "      <td>5.845354</td>\n",
       "      <td>0.391628</td>\n",
       "      <td>0.728963</td>\n",
       "      <td>6.293419</td>\n",
       "      <td>31.706102</td>\n",
       "      <td>145.969189</td>\n",
       "      <td>11.311909</td>\n",
       "      <td>255.023211</td>\n",
       "      <td>6.823125</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.304116</td>\n",
       "      <td>17.424774</td>\n",
       "      <td>0.545837</td>\n",
       "      <td>5.666628</td>\n",
       "      <td>3.296716</td>\n",
       "      <td>2.162280</td>\n",
       "      <td>1.195097</td>\n",
       "      <td>2.406729</td>\n",
       "      <td>2.211106</td>\n",
       "      <td>36.842611</td>\n",
       "      <td>...</td>\n",
       "      <td>5.873877</td>\n",
       "      <td>0.394348</td>\n",
       "      <td>0.752570</td>\n",
       "      <td>6.555357</td>\n",
       "      <td>31.768228</td>\n",
       "      <td>146.451896</td>\n",
       "      <td>7.129081</td>\n",
       "      <td>264.364066</td>\n",
       "      <td>6.886408</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2353</th>\n",
       "      <td>13.304122</td>\n",
       "      <td>17.424767</td>\n",
       "      <td>0.546071</td>\n",
       "      <td>5.883844</td>\n",
       "      <td>4.032418</td>\n",
       "      <td>1.994006</td>\n",
       "      <td>0.291268</td>\n",
       "      <td>2.424025</td>\n",
       "      <td>2.114058</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.564949</td>\n",
       "      <td>0.891262</td>\n",
       "      <td>1.334922</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>157.070869</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27.188622</td>\n",
       "      <td>3.195376</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2354</th>\n",
       "      <td>14.705903</td>\n",
       "      <td>14.565372</td>\n",
       "      <td>0.775868</td>\n",
       "      <td>5.762599</td>\n",
       "      <td>3.267801</td>\n",
       "      <td>2.247387</td>\n",
       "      <td>1.220651</td>\n",
       "      <td>2.485813</td>\n",
       "      <td>2.292618</td>\n",
       "      <td>13.437118</td>\n",
       "      <td>...</td>\n",
       "      <td>5.125079</td>\n",
       "      <td>0.586268</td>\n",
       "      <td>1.090520</td>\n",
       "      <td>6.842683</td>\n",
       "      <td>26.057229</td>\n",
       "      <td>155.863963</td>\n",
       "      <td>3.950551</td>\n",
       "      <td>96.533105</td>\n",
       "      <td>5.212012</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2355</th>\n",
       "      <td>15.584269</td>\n",
       "      <td>14.558553</td>\n",
       "      <td>0.996165</td>\n",
       "      <td>5.671568</td>\n",
       "      <td>3.309310</td>\n",
       "      <td>2.167403</td>\n",
       "      <td>1.257100</td>\n",
       "      <td>2.411225</td>\n",
       "      <td>2.216060</td>\n",
       "      <td>17.360722</td>\n",
       "      <td>...</td>\n",
       "      <td>5.541509</td>\n",
       "      <td>0.437346</td>\n",
       "      <td>1.043889</td>\n",
       "      <td>6.861711</td>\n",
       "      <td>5.733667</td>\n",
       "      <td>149.161023</td>\n",
       "      <td>5.171982</td>\n",
       "      <td>94.305802</td>\n",
       "      <td>5.134497</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2356</th>\n",
       "      <td>14.702534</td>\n",
       "      <td>14.565388</td>\n",
       "      <td>0.748758</td>\n",
       "      <td>5.756853</td>\n",
       "      <td>3.269475</td>\n",
       "      <td>2.240717</td>\n",
       "      <td>1.419059</td>\n",
       "      <td>2.478957</td>\n",
       "      <td>2.285506</td>\n",
       "      <td>12.934202</td>\n",
       "      <td>...</td>\n",
       "      <td>5.188433</td>\n",
       "      <td>0.624077</td>\n",
       "      <td>1.035548</td>\n",
       "      <td>6.842683</td>\n",
       "      <td>12.340549</td>\n",
       "      <td>158.114543</td>\n",
       "      <td>4.832410</td>\n",
       "      <td>75.087040</td>\n",
       "      <td>4.788936</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2357</th>\n",
       "      <td>14.702534</td>\n",
       "      <td>14.565388</td>\n",
       "      <td>0.748758</td>\n",
       "      <td>5.669619</td>\n",
       "      <td>3.269478</td>\n",
       "      <td>2.169232</td>\n",
       "      <td>1.420028</td>\n",
       "      <td>2.415078</td>\n",
       "      <td>2.219121</td>\n",
       "      <td>12.872076</td>\n",
       "      <td>...</td>\n",
       "      <td>5.215954</td>\n",
       "      <td>0.624077</td>\n",
       "      <td>1.071180</td>\n",
       "      <td>6.842683</td>\n",
       "      <td>12.340549</td>\n",
       "      <td>158.498543</td>\n",
       "      <td>4.832410</td>\n",
       "      <td>74.976625</td>\n",
       "      <td>4.787451</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2358 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       BCUTv-1l   BCUTi-1h  BCUTp-1l  BCUTZ-1l  BCUTd-1h  BCUTare-1l  \\\n",
       "0     14.701776  14.557741  0.744080  5.748825  3.218349    2.238180   \n",
       "1     14.700550  13.644287  0.741446  5.679853  3.258335    2.178923   \n",
       "2     13.304117  17.424774  0.545839  5.666628  3.296716    2.162280   \n",
       "3     13.304118  17.424774  0.545842  5.666628  3.296579    2.162280   \n",
       "4     13.304116  17.424774  0.545837  5.666628  3.296716    2.162280   \n",
       "...         ...        ...       ...       ...       ...         ...   \n",
       "2353  13.304122  17.424767  0.546071  5.883844  4.032418    1.994006   \n",
       "2354  14.705903  14.565372  0.775868  5.762599  3.267801    2.247387   \n",
       "2355  15.584269  14.558553  0.996165  5.671568  3.309310    2.167403   \n",
       "2356  14.702534  14.565388  0.748758  5.756853  3.269475    2.240717   \n",
       "2357  14.702534  14.565388  0.748758  5.669619  3.269478    2.169232   \n",
       "\n",
       "      BCUTs-1l  BCUTse-1l  BCUTpe-1l  PEOE_VSA9  ...     piPC4  \\\n",
       "0     1.240005   2.479451   2.284833  12.710848  ...  4.754667   \n",
       "1     1.143745   2.424758   2.228722  39.958147  ...  5.547129   \n",
       "2     1.195097   2.406729   2.211106  30.297854  ...  5.873877   \n",
       "3     1.195097   2.406729   2.211106  30.297854  ...  5.845354   \n",
       "4     1.195097   2.406729   2.211106  36.842611  ...  5.873877   \n",
       "...        ...        ...        ...        ...  ...       ...   \n",
       "2353  0.291268   2.424025   2.114058   0.000000  ...  2.564949   \n",
       "2354  1.220651   2.485813   2.292618  13.437118  ...  5.125079   \n",
       "2355  1.257100   2.411225   2.216060  17.360722  ...  5.541509   \n",
       "2356  1.419059   2.478957   2.285506  12.934202  ...  5.188433   \n",
       "2357  1.420028   2.415078   2.219121  12.872076  ...  5.215954   \n",
       "\n",
       "      ETA_dEpsilon_D   GATS1se     SRW09   SMR_VSA6      AATS1i    MDEC-22  \\\n",
       "0           0.492929  0.839738  6.580639   6.606882  151.382734   2.249577   \n",
       "1           0.364441  0.878124  7.412764  34.729217  144.347348   3.452842   \n",
       "2           0.391712  0.714820  6.555357  25.161346  145.867240   8.909374   \n",
       "3           0.391628  0.728963  6.293419  31.706102  145.969189  11.311909   \n",
       "4           0.394348  0.752570  6.555357  31.768228  146.451896   7.129081   \n",
       "...              ...       ...       ...        ...         ...        ...   \n",
       "2353        0.891262  1.334922  0.000000   0.000000  157.070869   0.000000   \n",
       "2354        0.586268  1.090520  6.842683  26.057229  155.863963   3.950551   \n",
       "2355        0.437346  1.043889  6.861711   5.733667  149.161023   5.171982   \n",
       "2356        0.624077  1.035548  6.842683  12.340549  158.114543   4.832410   \n",
       "2357        0.624077  1.071180  6.842683  12.340549  158.498543   4.832410   \n",
       "\n",
       "         VR1_DzZ  VR3_Dzpe  target  \n",
       "0      74.441211  4.780763       1  \n",
       "1     200.587788  6.399368       1  \n",
       "2     264.575821  6.887221       1  \n",
       "3     255.023211  6.823125       1  \n",
       "4     264.364066  6.886408       1  \n",
       "...          ...       ...     ...  \n",
       "2353   27.188622  3.195376       0  \n",
       "2354   96.533105  5.212012       0  \n",
       "2355   94.305802  5.134497       1  \n",
       "2356   75.087040  4.788936       0  \n",
       "2357   74.976625  4.787451       0  \n",
       "\n",
       "[2358 rows x 101 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('C:/Users/NEIST/Desktop/Pre/main_datasets/MI_dataset_100.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "Label = preprocessing.LabelEncoder()\n",
    "target = Label.fit_transform(data['target'])\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.target.values\n",
    "x_data = df.drop(['target'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize\n",
    "x = (x_data - np.min(x_data)) / (np.max(x_data) - np.min(x_data)).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.2,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train, x_val, y_train, y_val = train_test_split(x_train,y_train,test_size = 0.10,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transpose matrices\n",
    "x_train = x_train.T\n",
    "y_train = y_train.T\n",
    "x_test = x_test.T\n",
    "y_test = y_test.T\n",
    "#x_val = x_val.T\n",
    "#y_val = y_val.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANN using MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(10, 10, 10), max_iter=100)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "MLP = MLPClassifier(hidden_layer_sizes=(10,10,10), max_iter=100)\n",
    "MLP.fit(x_train.T, y_train.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator=MLP, X=x_train.T, y=y_train.T, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.77248677 0.78306878 0.84126984 0.83597884 0.81481481 0.7989418\n",
      " 0.82446809 0.79787234 0.79255319 0.75531915]\n",
      "0.8016773612518294\n"
     ]
    }
   ],
   "source": [
    "print(accuracies)\n",
    "print(accuracies.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84.32203389830508\n",
      "80.22269353128314\n"
     ]
    }
   ],
   "source": [
    "predictions = MLP.predict(x_test.T)\n",
    "acc = MLP.score(x_test.T,y_test.T)*100\n",
    "predictions_tr = MLP.predict(x_train.T)\n",
    "#predictions_val = MLP.predict(x_val.T)\n",
    "acc = MLP.score(x_test.T,y_test.T)*100\n",
    "acc_tr =MLP.score(x_train.T,y_train.T)*100\n",
    "#acc_val =MLP.score(x_val.T,y_val.T)*100\n",
    "print(acc)\n",
    "print(acc_tr)\n",
    "#print(acc_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5464393232204273"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import matthews_corrcoef\n",
    "matthews_corrcoef(y_test, predictions) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.metrics import matthews_corrcoef\n",
    "#matthews_corrcoef(y_val, predictions_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 64  52]\n",
      " [ 22 334]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.55      0.63       116\n",
      "           1       0.87      0.94      0.90       356\n",
      "\n",
      "    accuracy                           0.84       472\n",
      "   macro avg       0.80      0.74      0.77       472\n",
      "weighted avg       0.84      0.84      0.83       472\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cf =(confusion_matrix(y_test, predictions))\n",
    "#cf_val =(confusion_matrix(y_val, predictions_val))\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "print(classification_report(y_test, predictions))\n",
    "#print(confusion_matrix(y_train, predictions_tr))\n",
    "#print(classification_report(y_train, predictions_tr))\n",
    "#print(confusion_matrix(y_val, predictions_val))\n",
    "#print(classification_report(y_val, predictions_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2bb3cedf1f0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cf_plt =sns.heatmap(cf,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n",
    "cf_plt.set(xlabel = \"True Value\", ylabel =\"Actual Value\")\n",
    "cf_plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cf_plt =sns.heatmap(cf_val,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n",
    "#cf_plt.set(xlabel = \"True Value\", ylabel =\"Actual Value\")\n",
    "#cf_plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC = 0.745)\n",
      "R2 = 0.154)\n",
      "MSE = 0.157)\n",
      "MBE = 0.064)\n",
      "RMSE = 0.396)\n",
      "Variance = 0.149)\n",
      "Bias = 0.061)\n",
      "Kappa = 0.537)\n"
     ]
    }
   ],
   "source": [
    "#parameters\n",
    "r_auc_score = roc_auc_score(y_test, predictions)\n",
    "print('AUC = %0.3f)' % r_auc_score)\n",
    "rsquare = r2_score(y_test, predictions)\n",
    "print('R2 = %0.3f)' % rsquare)\n",
    "#mae = mean_absolute_error(y_test, predictions)\n",
    "#print('MAE = %0.3f)' % mae)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print('MSE = %0.3f)' % mse)\n",
    "mbe = np.mean(predictions - y_test)\n",
    "print('MBE = %0.3f)'% mbe)\n",
    "rmse = mse** (1/2)\n",
    "print('RMSE = %0.3f)' % rmse)\n",
    "variance = np.var(predictions)\n",
    "print('Variance = %0.3f)'% variance)\n",
    "sse = np.mean((np.mean(predictions) - y)** 2)\n",
    "bias = sse - variance\n",
    "print('Bias = %0.3f)'% bias)\n",
    "kappa = cohen_kappa_score(y_test, predictions)\n",
    "print('Kappa = %0.3f)'% kappa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.06179775 0.44827586]\n",
      "[0.44827586 0.06179775]\n",
      "[0.25581395 0.13471503]\n"
     ]
    }
   ],
   "source": [
    "#fall out\n",
    "FP = cf.sum(axis=0) - np.diag(cf) \n",
    "FN = cf.sum(axis=1) - np.diag(cf)\n",
    "TP = np.diag(cf)\n",
    "TN = cf.sum() - (FP + FN + TP)\n",
    "FP = FP.astype(float)\n",
    "FN = FN.astype(float)\n",
    "TP = TP.astype(float)\n",
    "TN = TN.astype(float)\n",
    "# Sensitivity, hit rate, recall, or true positive rate\n",
    "#TPR = TP/(TP+FN)\n",
    "# Specificity or true negative rate\n",
    "#TNR = TN/(TN+FP) \n",
    "# Precision or positive predictive value\n",
    "#PPV = TP/(TP+FP)\n",
    "# Negative predictive value\n",
    "#NPV = TN/(TN+FN)\n",
    "# Fall out or false positive rate\n",
    "FPR = FP/(FP+TN)\n",
    "# False negative rate\n",
    "FNR = FN/(TP+FN)\n",
    "# False discovery rate\n",
    "FDR = FP/(TP+FP)\n",
    "# Overall accuracy for each class\n",
    "#ACC = (TP+TN)/(TP+FP+FN+TN)\n",
    "print(FPR)\n",
    "print(FNR)\n",
    "print(FDR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete\n"
     ]
    }
   ],
   "source": [
    "print(\"complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
